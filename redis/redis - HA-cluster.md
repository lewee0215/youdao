# Redis 集群Cluster
http://doc.redisfans.com/topic/cluster-tutorial.html#redis-guarantee  
Redis 集群使用数据分片（sharding）而非一致性哈希（consistency hashing）来实现： 一个 Redis 集群包含 16384 个哈希槽（hash slot）， 数据库中的每个键都属于这 16384 个哈希槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和

## Redis Cluster 节点ID：
http://shift-alt-ctrl.iteye.com/blog/2285470  

每个节点在集群中由一个独一无二的ID标识，该ID是一个十六进制表示的160位随机数，在节点第一次启动时由/dev/urandom生成。
节点会将它的ID保存到配置文件，只要这个配置文件不被删除，节点就会一直沿用这个ID。一个节点可以改变它的IP和端口号，而不改变节点ID。
集群可以自动识别出IP/端口号的变化，并将这一信息通过Gossip协议广播给其他节点，节点信心包括：

a）节点所使用的IP地址和TCP端口号。
b）节点的标志（flags）。
c）节点负责处理的哈希槽。
d）节点最新一次使用集群连接发送PING数据包（packet）的时间。
e）节点最近一次在回复中接收到PONG数据包的时间。
f）集群将该节点标记为下线的时间。
g）该节点的从节点数量。

如果该节点是从节点的话，那么它会记录主节点的节点ID。如果这是一个主节点的话，那么主节点ID这一栏的值为0000000

### Cluster 架构细节
(1)所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.
(2)节点的fail是通过集群中超过半数的节点检测失效时才生效.
(3)客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可
(4)redis-cluster把所有的物理节点映射到[0-16383]slot上,cluster 负责维护node<->slot<->value

### Cluster 命令支持
> Redis 集群不支持那些需要同时处理多个键的 Redis 命令， 因为执行这些命令需要在多个 Redis 节点之间移动数据， 并且在高负载的情况下， 这些命令将降低 Redis 集群的性能， 并导致不可预测的行为  

Redis单实例支持的命令，Cluster也都支持，但是对于“multi-key”操作（即一次RPC调用中需要进行多个key的操作）比如Set类型的交集、并集等，则要求这些key必须属于同一个node。

Cluster不能进行跨Nodes操作，也没有nodes提供merge层代理。

Cluster中实现了一个称为“hash tags”的概念，每个key都可以包含一个自定义的“tags”，在存储时将根据tags计算此key应该分布在哪个nodes上（而不是使用key计算，但是存储层面仍然是key）；

但是在Cluster环境下，将不支持SELECT命令，所有的key都将保存在默认的database中。

1. 不支持多key操作
2. 如果一定要使用多key操作，请确保所有的key都在一个node上，具体方法是使用“hash tag”方案
hash tag方案是一种数据分布的例外情况

### redis-trib 命令
当我们有了六个正在运行中的 Redis 实例， 接下来我们需要使用这些实例来创建集群， 并为每个节点编写配置文件。
通过使用 Redis 集群命令行工具 redis-trib ， 编写节点配置文件的工作可以非常容易地完成： redis-trib 位于 Redis 源码的 src 文件夹中， 它是一个 Ruby 程序， 这个程序通过向实例发送特殊命令来完成创建新集群， 检查集群， 或者对集群进行重新分片（reshared）等工作。

### 哈希槽结构 
很容易添加或者删除节点. 比如如果我想新添加个节点D, 我需要从节点 A, B, C中得部分槽到D上. 如果我像移除节点A,需要将A中得槽移到B和C节点上,然后将没有任何槽的A节点从集群中移除即可. 由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态.
http://shift-alt-ctrl.iteye.com/blog/2285470
每一个数据的键被哈希函数映射到一个槽位：HASH_SLOT = CRC16(key) mod 16384。
Redis 集群没有并使用传统的一致性哈希来分配数据，而是采用另外一种叫做哈希槽 (hash slot)的方式来分配的。
是Redis的集群实现，内置数据自动分片机制，集群内部将所有的key映射到16384个Slot中，集群中的每个Redis Instance负责其中的一部分的Slot的读写。集群客户端连接集群中任一Redis Instance即可发送命令，当Redis Instance收到自己不负责的Slot的请求时，会将负责请求Key所在Slot的Redis Instance地址返回给客户端，客户端收到后自动将原请求重新发往这个地址，对外部透明。一个Key到底属于哪个Slot由crc16(key) % 16384 决定。

### hash tags
在计算hash slots时有一个意外的情况，用于支持“hash tags”；hash tags用于确保多个keys能够被分配在同一个hash slot中，用于支持multi-key操作。

关于负载均衡，集群的Redis Instance之间可以迁移数据，以Slot为单位，但不是自动的，需要外部命令触发。
需要注意的是：必须要3个或以上的主节点，否则在创建集群时会失败，并且当存活的主节点数小于总节点数的一半时，整个集群就无法提供服务了。

集群的节点(Redis Instance)和节点之间两两定期交换集群内节点信息并且更新，从发送节点的角度看，这些信息包括：集群内有哪些节点，IP和PORT是什么，节点名字是什么，节点的状态(比如OK，PFAIL，FAIL，后面详述)是什么，包括节点角色(master 或者 slave)等。
集群由N组主从Redis Instance组成。主可以没有从，但是没有从 意味着主宕机后主负责的Slot读写服务不可用。一个主可以有多个从，主宕机时，某个从会被提升为主，具体哪个从被提升为主，协议类似于Raft

### 数据迁移
http://itindex.net/detail/51378-redis-cluster-redis
Redis Cluster支持在线增/减节点。 基于桶的数据分布方式大大降低了迁移成本，只需将数据桶从一个Redis Node迁移到另一个Redis Node即可完成迁移。 
当桶从一个Node A向另一个Node B迁移时，Node A和Node B都会有这个桶，Node A上桶的状态设置为MIGRATING，Node B上桶的状态被设置为IMPORTING 
当客户端请求时： 所有在Node A上的请求都将由A来处理，所有不在A上的key都由Node B来处理。同时，Node A上将不会创建新的key 

http://www.cnblogs.com/zhoujinyi/p/6477133.html
port 7000
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
**  cluster-conf-file 选项则设定了保存节点配置文件的路径， 默认值为 nodes.conf.节点配置文件无须人为修改，它由 Redis 集群在启动时创建，在需要时自动进行更新。

### 集群一致性
https://blog.csdn.net/u011535541/article/details/78834565
主从和slot的一致性是由epoch来管理的. epoch就像Raft中的term, 但仅仅是像. 每个节点有一个自己独特的epoch和整个集群的epoch, 为简化下面都称为node epoch和cluster epoch. 
node epoch一直递增, 其表示某节点最后一次变成主节点或获取新slot所有权的逻辑时间. 
cluster epoch则是整个集群中最大的那个node epoch. 我们称递增node epoch为bump epoch, 它会用当前的cluster epoch加一来更新自己的node epoch.

### 失效检测
Redis集群失效检测是用来识别出大多数节点何时无法访问某一个主节点或从节点。
每个节点都有一份跟其他已知节点相关的标识列表。其中有两个标志是用于失效检测，分别是PFAIL和FAIL。PFAIL表示可能失效，这是一个非公认的失效类型。FAIL表示一个节点已经失效，而这个情况已经被大多数节点在，某段时间内确认过了。
PFAIL标识：
当一个节点在超过NODE_TIMEOUT时间后仍然无法访问某个节点（发送一个ping包已经等待了超过NODE_TIMEOUT时间，若是经过一半NODE_TIMEOUT时间还没收到回复，尝试重新连接），那么它会用PFAIL来标识这个不可达的节点。无论节点类型是什么，主节点和从节点都能标识其他节点为PFAIL。
FAIL标识：
单独一个PFAIL标识只是每个节点的一些关于其他节点的本地信息，它不是为了起作用而使用的，也不足够触发从节点的提升。要让一个节点被认为失效了，那需要让PFAIL上升为FAIL状态。前面提到过节点之间通过gossip消息来交互随机的已知节点的状态信息。最中每个节点都能收到一份其他每个节点标识。当下面条件满足时，PFAIL状态升级为FAIL：
某个节点A，标记另一个节点B为PFAIL。
节点A通过gossip字段收集到的集群中大部分主节点标识的B的状态信息。
大部分主节点标记B为PFAIL状态，或者在NODE_TIMEOUT*FAIL_REPORT_VALIDITY_MULT这个时间内是处于PFAIL状态。
如果以上条件都满足了，那么节点A会：
标记节点B为FAIL。
向所有节点发送一个FAIL消息。
FAIL消息会强制每个接收到这消息的节点把节点B标记为FAIL状态。
FAIL标识基本是单向的，一个节点能从PFAIL升级到FAIL状态，但要清除FAIL标识只有以下两种可能方法：
节点已经恢复可达，并且它是一个从节点。在这种情况下，FAIL标识可以清除掉，因为从节点并没有被故障转移。
节点已经恢复可达，而且他是一个主节点，但经过了很长时间（N*NODE_TIMEOUT）后也没有检查到任何从节点被提升了。
PFAIL->FAIL的转变使用一种弱协议（agreement）：
1）节点是在一段时间内收集其他几点信息，所以即使大多数主节点要去“同意”标记某节点为FAIL，实际上这只是表明说我们在不同时间里从不同节点收集了信息，得出当前的状态不一定是稳定的。
2）当每个节点检测到FAIL节点的时候会强制集群里的其他节点把各自对该节点的记录更新为FAIL，但没有一种方式能保证这个消息能到达所有节点。
然而Redis集群失效检测有一个要求：最终所有节点都应该同意给定节点的状态是FAIL，或者小部分节点相信该节点处于FAIL状态，或者相信节点不处于FAIL状态。在这两种情况中，最后集群都会认为给定的节点只有一个状态：
第一种情况：如果大多数节点都标记了某节点为FAIL，由于链条反应，这个主节点最终会被标记为FAIL。
第二种情况：当只有小部分的主节点标记某个节点为FAIL的时候，从节点的提升并不会发生，并且每个节点都会根据上面的清除规则（在经过了一段时间>N*NODE_TIMEOUT后仍然没有从节点提升，使用一个更正式的算法来保证每个节点最终都会知道节点的提升）清除FAIL状态。
本质上来说，FAIL标识只是用来触发从节点提升算法的安全部分。理论上一个从节点会在它的主节点不可达的时候独立起作用并且启动从节点提升程序，然后等待主节点来拒绝认可该提升。PFAIL-FAIL的状态变化、弱协议、强制在集群的可达部分用最短时间传播状态变更的FAIL消息，这些东西增加的复杂性有实际的好处。由于这种机制，如果集群处于错误状态时，所有节点都会在同一时间停止接收写入操作，者从使用redis集群的应用角度来看是个很好的特性。还有非必要的选举，是从节点在无法访问主节点时发起，若该节点能被其他大多数主节点访问的话，这个选举会被拒绝掉

### 主从节点选举
一旦某个主节点进入 FAIL 状态，如果主节点有一个或多个从节点存在，那么其中一个从节点会被升级为新的主节点，而其他从节点则会开始对这个新的主节点进行复制。
新的主节点由已下线主节点属下的所有从节点中自行选举产生，以下是选举的条件：
这个节点是已下线主节点的从节点。
已下线主节点负责处理的槽数量非空。
从节点的数据被认为是可靠的，也即是，主从节点之间的复制连接（replication link）的断线时长不能超过节点超时时限（node timeout）乘以REDIS_CLUSTER_SLAVE_VALIDITY_MULT 常量得出的积。
如果一个从节点满足了以上的所有条件，那么这个从节点将向集群中的其他主节点发送授权请求，询问它们，是否允许自己（从节点）升级为新的主节点。
如果发送授权请求的从节点满足以下属性，那么主节点将向从节点返回 FAILOVER_AUTH_GRANTED 授权，同意从节点的升级要求：
发送授权请求的是一个从节点，并且它所属的主节点处于 FAIL 状态。
在已下线主节点的所有从节点中，这个从节点的节点 ID 在排序中是最小的。
这个从节点处于正常的运行状态：它没有被标记为 FAIL 状态，也没有被标记为 PFAIL 状态。
一旦某个从节点在给定的时限内得到大部分主节点的授权，它就会开始执行以下故障转移操作：
通过 PONG 数据包（packet）告知其他节点，这个节点现在是主节点了。
通过 PONG 数据包告知其他节点，这个节点是一个已升级的从节点（promoted slave）。
接管（claiming）所有由已下线主节点负责处理的哈希槽。
显式地向所有节点广播一个 PONG 数据包，加速其他节点识别这个节点的进度，而不是等待定时的 PING / PONG 数据包。
所有其他节点都会根据新的主节点对配置进行相应的更新，特别地：
所有被新的主节点接管的槽会被更新。
已下线主节点的所有从节点会察觉到 PROMOTED 标志，并开始对新的主节点进行复制。
如果已下线的主节点重新回到上线状态，那么它会察觉到 PROMOTED 标志，并将自身调整为现任主节点的从节点。
在集群的生命周期中，如果一个带有 PROMOTED 标识的主节点因为某些原因转变成了从节点，那么该节点将丢失它所带有的 PROMOTED 标识。

### ASK重定向
https://blog.csdn.net/xmj_csdn/article/details/74908951
如果接收到ASK重定向，那么把查询的对象调整为指定的节点。
先发送ASKING命令，再开始发送查询。
现在不要更新本地客户端映射表。

当节点需要让一个客户端长期地（permanently）将针对某个槽的命令请求发送至另一个节点时，节点向客户端返回 MOVED 转向。
另一方面，当节点需要让客户端仅仅在下一个命令请求中转向至另一个节点时，节点向客户端返回 ASK 转向。

比如说槽 8 的例子中，因为槽 8 所包含的各个键分散在节点 A 和节点 B 中，所以当客户端在节点 A 中没找到某个键时，它应该转向到节点 B 中去寻找，但是这种转向应该仅仅影响一次命令查询，而不是让客户端每次都直接去查找节点 B ：在节点 A 所持有的属于槽 8 的键没有全部被迁移到节点 B 之前，客户端应该先访问节点 A ，然后再访问节点 B 。

如果我们要在查找节点 A 之后，继续查找节点 B ，那么客户端在向节点 B 发送命令请求之前，应该先发送一个 ASKING 命令，否则这个针对带有 IMPORTING 状态的槽的命令请求将被节点 B 拒绝执行。

接收到客户端 ASKING 命令的节点将为客户端设置一个一次性的标志（flag），使得客户端可以执行一次针对 IMPORTING 状态的槽的命令请求。
